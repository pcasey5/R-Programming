test_error_radial <- mean(test_pred_radial != test_data$Purchase)
cat("Training Error Rate (Radial Kernel):", train_error_radial, "\n")
cat("Test Error Rate (Radial Kernel):", test_error_radial, "\n")
tune.out <- tune(svm, Purchase ~ ., data = train_data, kernel = "radial",
ranges = list(cost = c( 0.01, 0.1, 1, 5, 10)))
svm_model_radial_optimal <- tune.out$best.model
summary(svm_model_radial_optimal)
#3----
#a)
set.seed(42)
library(ISLR2)
library(e1071)
data = OJ
train_indices <- sample(1:nrow(OJ), 800)
train_data <- OJ[train_indices, ]
test_data <- OJ[-train_indices, ]
#b)
svm_model_linear <- svm(Purchase ~ ., data = train_data, kernel = "linear", cost = 0.01)
summary(svm_model_linear)
#c)
train_pred_linear <- predict(svm_model_linear, newdata = train_data)
test_pred_linear <- predict(svm_model_linear, newdata = test_data)
train_error_linear <- mean(train_pred_linear != train_data$Purchase)
test_error_linear <- mean(test_pred_linear != test_data$Purchase)
cat("Training Error Rate (Linear Kernel):", train_error_linear, "\n")
cat("Test Error Rate (Linear Kernel):", test_error_linear, "\n")
#d)
tune.out <- tune(svm, Purchase ~ ., data = train_data, kernel = "linear",
ranges = list(cost = c( 0.01, 0.1, 1, 5, 10)))
svm_model_linear_optimal <- tune.out$best.model
summary(svm_model_linear_optimal)
train_pred_linear_optimal <- predict(svm_model_linear_optimal, newdata = train_data)
train_error_optimal <- mean(train_pred_linear_optimal != train_data$Purchase)
test_pred_optimal <- predict(svm_model_linear_optimal, newdata = test_data)
test_error_optimal <- mean(test_pred_optimal != test_data$Purchase)
cat("Training Error Rate (Optimal Linear Kernel):", train_error_optimal, "\n")
cat("Test Error Rate (Optimal Linear Kernel):", test_error_optimal, "\n")
#f)
svm_model_radial <- svm(Purchase ~ ., data = train_data, kernel = "radial", cost = 0.01)
summary(svm_model_radial)
train_pred_radial <- predict(svm_model_radial, newdata = train_data)
test_pred_radial <- predict(svm_model_radial, newdata = test_data)
train_error_radial <- mean(train_pred_radial != train_data$Purchase)
test_error_radial <- mean(test_pred_radial != test_data$Purchase)
cat("Training Error Rate (Radial Kernel):", train_error_radial, "\n")
cat("Test Error Rate (Radial Kernel):", test_error_radial, "\n")
tune.out <- tune(svm, Purchase ~ ., data = train_data, kernel = "radial",
ranges = list(cost = c( 0.01, 0.1, 1, 5, 10)))
svm_model_radial_optimal <- tune.out$best.model
summary(svm_model_radial_optimal)
train_pred_linear_optimal <- predict(svm_model_radial_optimal, newdata = train_data)
train_error_optimal <- mean(train_pred_radial_optimal != train_data$Purchase)
cat("Training Error Rate (Polynomial Kernel):", train_error_poly, "\n")
cat("Test Error Rate (Polynomial Kernel):", test_error_poly, "\n")
tune.out_poly <- tune(svm, Purchase ~ ., data = train_data, kernel = "polynomial", degree = 2,
ranges = list(cost = c(0.01, 0.1, 1, 5, 10)))
svm_model_poly_optimal <- tune.out_poly$best.model
summary(svm_model_poly_optimal)
train_pred_poly_optimal <- predict(svm_model_poly_optimal, newdata = train_data)
train_error_poly_optimal <- mean(train_pred_poly_optimal != train_data$Purchase)
svm_model_poly <- svm(Purchase ~ ., data = train_data, kernel = "polynomial", degree = 2, cost = 0.01)
summary(svm_model_poly)
train_pred_poly <- predict(svm_model_poly, newdata = train_data)
train_error_poly <- mean(train_pred_poly != train_data$Purchase)
test_pred_poly <- predict(svm_model_poly, newdata = test_data)
test_error_poly <- mean(test_pred_poly != test_data$Purchase)
cat("Training Error Rate (Polynomial Kernel):", train_error_poly, "\n")
cat("Test Error Rate (Polynomial Kernel):", test_error_poly, "\n")
tune.out_poly <- tune(svm, Purchase ~ ., data = train_data, kernel = "polynomial", degree = 2,
ranges = list(cost = c(0.01, 0.1, 1, 5, 10)))
svm_model_poly_optimal <- tune.out_poly$best.model
summary(svm_model_poly_optimal)
train_pred_poly_optimal <- predict(svm_model_poly_optimal, newdata = train_data)
train_error_poly_optimal <- mean(train_pred_poly_optimal != train_data$Purchase)
test_pred_poly_optimal <- predict(svm_model_poly_optimal, newdata = test_data)
test_error_poly_optimal <- mean(test_pred_poly_optimal != test_data$Purchase)
cat("Training Error Rate (Optimal Polynomial Kernel):", train_error_poly_optimal, "\n")
cat("Test Error Rate (Optimal Polynomial Kernel):", test_error_poly_optimal, "\n")
summary(svm_model_poly_optimal)
#h)
cat("Training Error Rate (Optimal Linear Kernel):", train_error_optimal, "\n")
cat("Test Error Rate (Optimal Linear Kernel):", test_error_optimal, "\n")
cat("Training Error Rate (Optimal Radial Kernel):", train_error_optimal, "\n")
cat("Test Error Rate (Optimal Radial Kernel):", test_error_optimal, "\n")
cat("Training Error Rate (Optimal Polynomial Kernel):", train_error_poly_optimal, "\n")
cat("Test Error Rate (Optimal Polynomial Kernel):", test_error_poly_optimal, "\n")
#3----
#a)
set.seed(42)
library(ISLR2)
library(e1071)
data = OJ
train_indices <- sample(1:nrow(OJ), 800)
train_data <- OJ[train_indices, ]
test_data <- OJ[-train_indices, ]
#b)
svm_model_linear <- svm(Purchase ~ ., data = train_data, kernel = "linear", cost = 0.01)
summary(svm_model_linear)
#c)
train_pred_linear <- predict(svm_model_linear, newdata = train_data)
test_pred_linear <- predict(svm_model_linear, newdata = test_data)
train_error_linear <- mean(train_pred_linear != train_data$Purchase)
test_error_linear <- mean(test_pred_linear != test_data$Purchase)
cat("Training Error Rate (Linear Kernel):", train_error_linear, "\n")
cat("Test Error Rate (Linear Kernel):", test_error_linear, "\n")
#d)
tune.out <- tune(svm, Purchase ~ ., data = train_data, kernel = "linear",
ranges = list(cost = c( 0.01, 0.1, 1, 5, 10)))
svm_model_linear_optimal <- tune.out$best.model
summary(svm_model_linear_optimal)
train_pred_linear_optimal <- predict(svm_model_linear_optimal, newdata = train_data)
train_error_optimal_linear <- mean(train_pred_linear_optimal != train_data$Purchase)
test_pred_optimal <- predict(svm_model_linear_optimal, newdata = test_data)
test_error_optimal_linear <- mean(test_pred_optimal != test_data$Purchase)
cat("Training Error Rate (Optimal Linear Kernel):", train_error_optimal_linear, "\n")
cat("Test Error Rate (Optimal Linear Kernel):", test_error_optimal_linear, "\n")
#f)
svm_model_radial <- svm(Purchase ~ ., data = train_data, kernel = "radial", cost = 0.01)
summary(svm_model_radial)
train_pred_radial <- predict(svm_model_radial, newdata = train_data)
test_pred_radial <- predict(svm_model_radial, newdata = test_data)
train_error_radial <- mean(train_pred_radial != train_data$Purchase)
test_error_radial <- mean(test_pred_radial != test_data$Purchase)
cat("Training Error Rate (Radial Kernel):", train_error_radial, "\n")
cat("Test Error Rate (Radial Kernel):", test_error_radial, "\n")
tune.out <- tune(svm, Purchase ~ ., data = train_data, kernel = "radial",
ranges = list(cost = c( 0.01, 0.1, 1, 5, 10)))
svm_model_radial_optimal <- tune.out$best.model
summary(svm_model_radial_optimal)
train_pred_linear_optimal <- predict(svm_model_radial_optimal, newdata = train_data)
train_error_optimal_kernel <- mean(train_pred_radial_optimal != train_data$Purchase)
#3----
#a)
set.seed(42)
library(ISLR2)
library(e1071)
data = OJ
train_indices <- sample(1:nrow(OJ), 800)
train_data <- OJ[train_indices, ]
test_data <- OJ[-train_indices, ]
#b)
svm_model_linear <- svm(Purchase ~ ., data = train_data, kernel = "linear", cost = 0.01)
summary(svm_model_linear)
#c)
train_pred_linear <- predict(svm_model_linear, newdata = train_data)
test_pred_linear <- predict(svm_model_linear, newdata = test_data)
train_error_linear <- mean(train_pred_linear != train_data$Purchase)
test_error_linear <- mean(test_pred_linear != test_data$Purchase)
cat("Training Error Rate (Linear Kernel):", train_error_linear, "\n")
cat("Test Error Rate (Linear Kernel):", test_error_linear, "\n")
#d)
tune.out <- tune(svm, Purchase ~ ., data = train_data, kernel = "linear",
ranges = list(cost = c( 0.01, 0.1, 1, 5, 10)))
svm_model_linear_optimal <- tune.out$best.model
summary(svm_model_linear_optimal)
train_pred_linear_optimal <- predict(svm_model_linear_optimal, newdata = train_data)
train_error_optimal_linear <- mean(train_pred_linear_optimal != train_data$Purchase)
test_pred_optimal <- predict(svm_model_linear_optimal, newdata = test_data)
test_error_optimal_linear <- mean(test_pred_optimal != test_data$Purchase)
cat("Training Error Rate (Optimal Linear Kernel):", train_error_optimal_linear, "\n")
cat("Test Error Rate (Optimal Linear Kernel):", test_error_optimal_linear, "\n")
#f)
svm_model_radial <- svm(Purchase ~ ., data = train_data, kernel = "radial", cost = 0.01)
summary(svm_model_radial)
train_pred_radial <- predict(svm_model_radial, newdata = train_data)
test_pred_radial <- predict(svm_model_radial, newdata = test_data)
train_error_radial <- mean(train_pred_radial != train_data$Purchase)
test_error_radial <- mean(test_pred_radial != test_data$Purchase)
cat("Training Error Rate (Radial Kernel):", train_error_radial, "\n")
cat("Test Error Rate (Radial Kernel):", test_error_radial, "\n")
tune.out <- tune(svm, Purchase ~ ., data = train_data, kernel = "radial",
ranges = list(cost = c( 0.01, 0.1, 1, 5, 10)))
svm_model_radial_optimal <- tune.out$best.model
summary(svm_model_radial_optimal)
train_pred_radial_optimal <- predict(svm_model_radial_optimal, newdata = train_data)
train_error_optimal_radial <- mean(train_pred_radial_optimal != train_data$Purchase)
test_pred_optimal <- predict(svm_model_radial_optimal, newdata = test_data)
test_error_optimal_radial <- mean(test_pred_optimal != test_data$Purchase)
cat("Training Error Rate (Optimal Radial Kernel):", train_error_optimal_radial, "\n")
cat("Test Error Rate (Optimal Radial Kernel):", test_error_optimal_radial, "\n")
svm_model_poly <- svm(Purchase ~ ., data = train_data, kernel = "polynomial", degree = 2, cost = 0.01)
summary(svm_model_poly)
train_pred_poly <- predict(svm_model_poly, newdata = train_data)
train_error_poly <- mean(train_pred_poly != train_data$Purchase)
test_pred_poly <- predict(svm_model_poly, newdata = test_data)
test_error_poly <- mean(test_pred_poly != test_data$Purchase)
cat("Training Error Rate (Polynomial Kernel):", train_error_poly, "\n")
cat("Test Error Rate (Polynomial Kernel):", test_error_poly, "\n")
tune.out_poly <- tune(svm, Purchase ~ ., data = train_data, kernel = "polynomial", degree = 2,
ranges = list(cost = c(0.01, 0.1, 1, 5, 10)))
svm_model_poly_optimal <- tune.out_poly$best.model
summary(svm_model_poly_optimal)
train_pred_poly_optimal <- predict(svm_model_poly_optimal, newdata = train_data)
train_error_poly_optimal <- mean(train_pred_poly_optimal != train_data$Purchase)
test_pred_poly_optimal <- predict(svm_model_poly_optimal, newdata = test_data)
test_error_poly_optimal <- mean(test_pred_poly_optimal != test_data$Purchase)
cat("Training Error Rate (Optimal Polynomial Kernel):", train_error_poly_optimal, "\n")
cat("Test Error Rate (Optimal Polynomial Kernel):", test_error_poly_optimal, "\n")
#h)
cat("Training Error Rate (Optimal Linear Kernel):", train_error_optimal, "\n")
#h)
cat("Training Error Rate (Optimal Linear Kernel):", train_error_optimal_linear, "\n")
cat("Test Error Rate (Optimal Linear Kernel):", test_error_optimal_linear, "\n")
cat("Training Error Rate (Optimal Radial Kernel):", train_error_optimal_radial, "\n")
cat("Test Error Rate (Optimal Radial Kernel):", test_error_optimal_radial, "\n")
cat("Training Error Rate (Optimal Polynomial Kernel):", train_error_poly_optimal, "\n")
cat("Test Error Rate (Optimal Polynomial Kernel):", test_error_poly_optimal, "\n")
#4----
library(ISLR2)
library(keras)
xdata <- data.matrix(
NYSE[, c("DJ_return", "log_volume","log_volatility")]
)
istrain <- NYSE[, "train"]
xdata <- scale(xdata)
lagm <- function(x, k = 1) {
n <- nrow(x)
pad <- matrix(NA, k, ncol(x))
rbind(pad, x[1:(n - k), ])
}
arframe <- data.frame(log_volume = xdata[, "log_volume"],
L1 = lagm(xdata, 1), L2 = lagm(xdata, 2),
L3 = lagm(xdata, 3), L4 = lagm(xdata, 4),
L5 = lagm(xdata, 5)
)
arframe <- arframe[-(1:5), ]
istrain <- istrain[-(1:5)]
arfit <- lm(log_volume ~ ., data = arframe[istrain, ])
arpred <- predict(arfit, arframe[!istrain, ])
V0 <- var(arframe[!istrain, "log_volume"])
1 - mean((arpred - arframe[!istrain, "log_volume"])**2) / V0
arframed <-
data.frame(day = NYSE[-(1:5), "day_of_week"], arframe)
arfitd <- lm(log_volume ~ ., data = arframed[istrain, ])
arpredd <- predict(arfitd, arframed[!istrain, ])
1 - mean((arpredd - arframe[!istrain, "log_volume"])**2) / V0
n <- nrow(arframe)
xrnn <- data.matrix(arframe[, -1])
xrnn <- array(xrnn, c(n, 3, 5))
xrnn <- xrnn[,, 5:1]
xrnn <- aperm(xrnn, c(1, 3, 2))
dim(xrnn)
model <- keras_model_sequential() %>%
layer_simple_rnn(units = 12,
input_shape = list(5, 3),
dropout = 0.1, recurrent_dropout = 0.1) %>%
layer_dense(units = 1)
########################
library(ISLR2)
library(keras)
# Load data
xdata <- data.matrix(NYSE[, c("DJ_return", "log_volume", "log_volatility", "day_of_week")])
istrain <- NYSE[, "train"]
xdata <- scale(xdata)
# Lag function
lagm <- function(x, k = 1) {
n <- nrow(x)
pad <- matrix(NA, k, ncol(x))
rbind(pad, x[1:(n - k), ])
}
# Create lagged features
arframe <- data.frame(
log_volume = xdata[, "log_volume"],
L1 = lagm(xdata, 1), L2 = lagm(xdata, 2),
L3 = lagm(xdata, 3), L4 = lagm(xdata, 4),
L5 = lagm(xdata, 5),
day_of_week = xdata[, "day_of_week"]
)
arframe <- arframe[-(1:5), ]
istrain <- istrain[-(1:5)]
# Fit linear regression model
arfit <- lm(log_volume ~ ., data = arframe[istrain, ])
arpred <- predict(arfit, arframe[!istrain, ])
V0 <- var(arframe[!istrain, "log_volume"])
1 - mean((arpred - arframe[!istrain, "log_volume"])^2) / V0
# Create a new dataframe with day_of_week
arframed <- data.frame(day_of_week = NYSE[-(1:5), "day_of_week"], arframe)
arfitd <- lm(log_volume ~ ., data = arframed[istrain, ])
arpredd <- predict(arfitd, arframed[!istrain, ])
1 - mean((arpredd - arframe[!istrain, "log_volume"])^2) / V0
# Prepare data for RNN
n <- nrow(arframe)
xrnn <- data.matrix(arframe[, -1])
xrnn <- array(xrnn, c(n, 3, 5))
xrnn <- xrnn[, , 5:1]
xrnn <- aperm(xrnn, c(1, 3, 2))
# Define RNN model
model <- keras_model_sequential() %>%
layer_simple_rnn(units = 12,
input_shape = list(5, 4),
dropout = 0.1, recurrent_dropout = 0.1) %>%
layer_dense(units = 1)
model %>% compile(optimizer = optimizer_rmsprop(),
loss = "mse")
# Fit RNN model
history <- model %>% fit(
xrnn[istrain, , ], arframe[istrain, "log_volume"],
batch_size = 64, epochs = 75,
validation_data = list(xrnn[!istrain, , ], arframe[!istrain, "log_volume"])
)
########################
# Load data
xdata <- data.matrix(NYSE[, c("DJ_return", "log_volume", "log_volatility", "day_of_week")])
istrain <- NYSE[, "train"]
xdata <- scale(xdata)
# Lag function
lagm <- function(x, k = 1) {
n <- nrow(x)
pad <- matrix(NA, k, ncol(x))
rbind(pad, x[1:(n - k), ])
}
# Create lagged features
arframe <- data.frame(
log_volume = xdata[, "log_volume"],
L1 = lagm(xdata, 1), L2 = lagm(xdata, 2),
L3 = lagm(xdata, 3), L4 = lagm(xdata, 4),
L5 = lagm(xdata, 5),
day_of_week = xdata[, "day_of_week"]
)
arframe <- arframe[-(1:5), ]
istrain <- istrain[-(1:5)]
# Fit linear regression model
arfit <- lm(log_volume ~ ., data = arframe[istrain, ])
arpred <- predict(arfit, arframe[!istrain, ])
V0 <- var(arframe[!istrain, "log_volume"])
1 - mean((arpred - arframe[!istrain, "log_volume"])^2) / V0
# Create a new dataframe with day_of_week
arframed <- data.frame(day_of_week = NYSE[-(1:5), "day_of_week"], arframe)
arfitd <- lm(log_volume ~ ., data = arframed[istrain, ])
arpredd <- predict(arfitd, arframed[!istrain, ])
1 - mean((arpredd - arframe[!istrain, "log_volume"])^2) / V0
# Prepare data for RNN
n <- nrow(arframe)
xrnn <- data.matrix(arframe[, -1])
xrnn <- array(xrnn, c(n, 4, 5))  # Adjusted to match the correct input shape
xrnn <- xrnn[, , 5:1]
xrnn <- aperm(xrnn, c(1, 3, 2))
# Define RNN model
model <- keras_model_sequential() %>%
layer_simple_rnn(units = 12,
input_shape = list(5, 4),  # Adjusted to match the correct input shape
dropout = 0.1, recurrent_dropout = 0.1) %>%
layer_dense(units = 1)
model %>% compile(optimizer = optimizer_rmsprop(),
loss = "mse")
# Fit RNN model
history <- model %>% fit(
xrnn[istrain, , ], arframe[istrain, "log_volume"],
batch_size = 64, epochs = 75,
validation_data = list(xrnn[!istrain, , ], arframe[!istrain, "log_volume"])
)
# Compute R2 for RNN model
kpred <- predict(model, xrnn[!istrain, , ])
1 - mean((kpred - arframe[!istrain, "log_volume"])^2) / V0
# Flatten data for the second RNN model
model <- keras_model_sequential() %>%
layer_flatten(input_shape = c(5, 4)) %>%
layer_dense(units = 1)
x <- model.matrix(log_volume ~ . - 1, data = arframed)
colnames(x)
# Define second RNN model
arnnd <- keras_model_sequential() %>%
layer_dense(units = 32, activation = 'relu',
input_shape = ncol(x)) %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = 1)
arnnd %>% compile(loss = "mse",
optimizer = optimizer_rmsprop())
# Fit second RNN model
history <- arnnd %>% fit(
x[istrain, ], arframe[istrain, "log_volume"], epochs = 30,
batch_size = 32, validation_data = list(x[!istrain, ], arframe[!istrain, "log_volume"])
)
# Plot training history
plot(history)
# Predict using second RNN model
npred <- predict(arnnd, x[!istrain, ])
1 - mean((arframe[!istrain, "log_volume"] - npred)^2) / V0
#4----
xdata <- data.matrix(NYSE[, c("DJ_return", "log_volume", "log_volatility", "day_of_week")])
istrain <- NYSE[, "train"]
xdata <- scale(xdata)
lagm <- function(x, k = 1) {
n <- nrow(x)
pad <- matrix(NA, k, ncol(x))
rbind(pad, x[1:(n - k), ])
}
arframe <- data.frame(
log_volume = xdata[, "log_volume"],
L1 = lagm(xdata, 1), L2 = lagm(xdata, 2),
L3 = lagm(xdata, 3), L4 = lagm(xdata, 4),
L5 = lagm(xdata, 5),
day_of_week = xdata[, "day_of_week"]
)
arframe <- arframe[-(1:5), ]
istrain <- istrain[-(1:5)]
arfit <- lm(log_volume ~ ., data = arframe[istrain, ])
arpred <- predict(arfit, arframe[!istrain, ])
V0 <- var(arframe[!istrain, "log_volume"])
1 - mean((arpred - arframe[!istrain, "log_volume"])^2) / V0
arframed <- data.frame(day_of_week = NYSE[-(1:5), "day_of_week"], arframe)
arfitd <- lm(log_volume ~ ., data = arframed[istrain, ])
arpredd <- predict(arfitd, arframed[!istrain, ])
1 - mean((arpredd - arframe[!istrain, "log_volume"])^2) / V0
n <- nrow(arframe)
xrnn <- data.matrix(arframe[, -1])
xrnn <- array(xrnn, c(n, 4, 5))  # Adjusted to match the correct input shape
xrnn <- xrnn[, , 5:1]
xrnn <- aperm(xrnn, c(1, 3, 2))
model <- keras_model_sequential() %>%
layer_simple_rnn(units = 12,
input_shape = list(5, 4),  # Adjusted to match the correct input shape
dropout = 0.1, recurrent_dropout = 0.1) %>%
layer_dense(units = 1)
model %>% compile(optimizer = optimizer_rmsprop(),
loss = "mse")
history <- model %>% fit(
xrnn[istrain, , ], arframe[istrain, "log_volume"],
batch_size = 64, epochs = 75,
validation_data = list(xrnn[!istrain, , ], arframe[!istrain, "log_volume"])
)
kpred <- predict(model, xrnn[!istrain, , ])
r2 = 1 - mean((kpred - arframe[!istrain, "log_volume"])^2) / V0
model <- keras_model_sequential() %>%
layer_flatten(input_shape = c(5, 4)) %>%
layer_dense(units = 1)
x <- model.matrix(log_volume ~ . - 1, data = arframed)
colnames(x)
arnnd <- keras_model_sequential() %>%
layer_dense(units = 32, activation = 'relu',
input_shape = ncol(x)) %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = 1)
arnnd %>% compile(loss = "mse",
optimizer = optimizer_rmsprop())
history <- arnnd %>% fit(
x[istrain, ], arframe[istrain, "log_volume"], epochs = 30,
batch_size = 32, validation_data = list(x[!istrain, ], arframe[!istrain, "log_volume"])
)
# Plot training history
plot(history)
# Predict using second RNN model
npred <- predict(arnnd, x[!istrain, ])
1 - mean((arframe[!istrain, "log_volume"] - npred)^2) / V0
print(r2)
print(r2_second)
#4----
xdata <- data.matrix(NYSE[, c("DJ_return", "log_volume", "log_volatility", "day_of_week")])
istrain <- NYSE[, "train"]
xdata <- scale(xdata)
lagm <- function(x, k = 1) {
n <- nrow(x)
pad <- matrix(NA, k, ncol(x))
rbind(pad, x[1:(n - k), ])
}
arframe <- data.frame(
log_volume = xdata[, "log_volume"],
L1 = lagm(xdata, 1), L2 = lagm(xdata, 2),
L3 = lagm(xdata, 3), L4 = lagm(xdata, 4),
L5 = lagm(xdata, 5),
day_of_week = xdata[, "day_of_week"]
)
arframe <- arframe[-(1:5), ]
istrain <- istrain[-(1:5)]
arfit <- lm(log_volume ~ ., data = arframe[istrain, ])
arpred <- predict(arfit, arframe[!istrain, ])
V0 <- var(arframe[!istrain, "log_volume"])
1 - mean((arpred - arframe[!istrain, "log_volume"])^2) / V0
arframed <- data.frame(day_of_week = NYSE[-(1:5), "day_of_week"], arframe)
arfitd <- lm(log_volume ~ ., data = arframed[istrain, ])
arpredd <- predict(arfitd, arframed[!istrain, ])
1 - mean((arpredd - arframe[!istrain, "log_volume"])^2) / V0
n <- nrow(arframe)
xrnn <- data.matrix(arframe[, -1])
xrnn <- array(xrnn, c(n, 4, 5))  # Adjusted to match the correct input shape
xrnn <- xrnn[, , 5:1]
xrnn <- aperm(xrnn, c(1, 3, 2))
model <- keras_model_sequential() %>%
layer_simple_rnn(units = 12,
input_shape = list(5, 4),  # Adjusted to match the correct input shape
dropout = 0.1, recurrent_dropout = 0.1) %>%
layer_dense(units = 1)
model %>% compile(optimizer = optimizer_rmsprop(),
loss = "mse")
history <- model %>% fit(
xrnn[istrain, , ], arframe[istrain, "log_volume"],
batch_size = 64, epochs = 75,
validation_data = list(xrnn[!istrain, , ], arframe[!istrain, "log_volume"])
)
kpred <- predict(model, xrnn[!istrain, , ])
r2 = 1 - mean((kpred - arframe[!istrain, "log_volume"])^2) / V0
model <- keras_model_sequential() %>%
layer_flatten(input_shape = c(5, 4)) %>%
layer_dense(units = 1)
x <- model.matrix(log_volume ~ . - 1, data = arframed)
colnames(x)
arnnd <- keras_model_sequential() %>%
layer_dense(units = 32, activation = 'relu',
input_shape = ncol(x)) %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = 1)
arnnd %>% compile(loss = "mse",
optimizer = optimizer_rmsprop())
history <- arnnd %>% fit(
x[istrain, ], arframe[istrain, "log_volume"], epochs = 30,
batch_size = 32, validation_data = list(x[!istrain, ], arframe[!istrain, "log_volume"])
)
plot(history)
npred <- predict(arnnd, x[!istrain, ])
r2_second = 1 - mean((arframe[!istrain, "log_volume"] - npred)^2) / V0
print(r2)
print(r2_second)
